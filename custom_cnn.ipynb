{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRZ87Vti6agH"
      },
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imread\n",
        "import time\n",
        "import random\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# for auto-reloading external modules\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading external modules\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading and processing utilities\n",
        "\n",
        "def load_CIFAR10(ROOT):\n",
        "    \"\"\"\n",
        "    Load CIFAR-10 dataset\n",
        "    \"\"\"\n",
        "    import os, pickle\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for b in range(1, 6):\n",
        "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
        "        X, Y = load_CIFAR_batch(f)\n",
        "        xs.append(X)\n",
        "        ys.append(Y)\n",
        "    Xtr = np.concatenate(xs)\n",
        "    Ytr = np.concatenate(ys)\n",
        "    del X, Y\n",
        "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
        "    return Xtr, Ytr, Xte, Yte\n",
        "\n",
        "def load_CIFAR_batch(filename):\n",
        "    \"\"\" load single batch of cifar \"\"\"\n",
        "    import pickle\n",
        "    with open(filename, 'rb') as f:\n",
        "        datadict = pickle.load(f, encoding='latin1')\n",
        "        X = datadict['data']\n",
        "        Y = datadict['labels']\n",
        "        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype(\"float\")\n",
        "        Y = np.array(Y)\n",
        "        return X, Y\n",
        "\n",
        "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for classifiers. These are the same steps as we used for the SVM, but\n",
        "    condensed to a single function.\n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "\n",
        "    # Subsample the data\n",
        "    mask = list(range(num_training, num_training + num_validation))\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = list(range(num_training))\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = list(range(num_test))\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "\n",
        "    # Normalize the data: subtract the mean image\n",
        "    mean_image = np.mean(X_train, axis=0)\n",
        "    X_train -= mean_image\n",
        "    X_val -= mean_image\n",
        "    X_test -= mean_image\n",
        "\n",
        "    # Transpose so that channels come first\n",
        "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n",
        "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n",
        "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n",
        "\n",
        "    # Package data into dictionaries\n",
        "    return {\n",
        "        'X_train': X_train, 'y_train': y_train,\n",
        "        'X_val': X_val, 'y_val': y_val,\n",
        "        'X_test': X_test, 'y_test': y_test,\n",
        "    }"
      ],
      "metadata": {
        "id": "HFQJBcUs6dA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN layers implementation\n",
        "\n",
        "def conv_forward_naive(x, w, b, conv_param):\n",
        "    \"\"\"\n",
        "    A naive implementation of the forward pass for a convolutional layer.\n",
        "\n",
        "    The input consists of N data points, each with C channels, height H and\n",
        "    width W. We convolve each input with F different filters, where each filter\n",
        "    spans all C channels and has height HH and width WW.\n",
        "\n",
        "    Input:\n",
        "    - x: Input data of shape (N, C, H, W)\n",
        "    - w: Filter weights of shape (F, C, HH, WW)\n",
        "    - b: Biases, of shape (F,)\n",
        "    - conv_param: A dictionary with the following keys:\n",
        "      - 'stride': The number of pixels between adjacent receptive fields in the\n",
        "        horizontal and vertical directions.\n",
        "      - 'pad': The number of pixels that will be used to zero-pad the input.\n",
        "\n",
        "    During padding, 'pad' zeros should be placed symmetrically (i.e equally on both sides)\n",
        "    along the height and width axes of the input. Be careful not to modify the original\n",
        "    input x directly.\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
        "      H' = 1 + (H + 2 * pad - HH) / stride\n",
        "      W' = 1 + (W + 2 * pad - WW) / stride\n",
        "    - cache: (x, w, b, conv_param)\n",
        "    \"\"\"\n",
        "    stride = conv_param.get('stride', 1)\n",
        "    pad = conv_param.get('pad', 0)\n",
        "\n",
        "    N, C, H, W = x.shape\n",
        "    F, _, HH, WW = w.shape\n",
        "\n",
        "    H_out = 1 + (H + 2 * pad - HH) // stride\n",
        "    W_out = 1 + (W + 2 * pad - WW) // stride\n",
        "\n",
        "    out = np.zeros((N, F, H_out, W_out))\n",
        "\n",
        "    # Pad the input\n",
        "    x_padded = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant')\n",
        "\n",
        "    for n in range(N):  # For each example\n",
        "        for f in range(F):  # For each filter\n",
        "            for h_out in range(H_out):\n",
        "                for w_out in range(W_out):\n",
        "                    h_start = h_out * stride\n",
        "                    w_start = w_out * stride\n",
        "                    x_slice = x_padded[n, :, h_start:h_start+HH, w_start:w_start+WW]\n",
        "                    out[n, f, h_out, w_out] = np.sum(x_slice * w[f]) + b[f]\n",
        "\n",
        "    cache = (x, w, b, conv_param)\n",
        "    return out, cache\n",
        "\n",
        "def conv_backward_naive(dout, cache):\n",
        "    \"\"\"\n",
        "    A naive implementation of the backward pass for a convolutional layer.\n",
        "\n",
        "    Inputs:\n",
        "    - dout: Upstream derivatives.\n",
        "    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x\n",
        "    - dw: Gradient with respect to w\n",
        "    - db: Gradient with respect to b\n",
        "    \"\"\"\n",
        "    x, w, b, conv_param = cache\n",
        "\n",
        "    stride = conv_param.get('stride', 1)\n",
        "    pad = conv_param.get('pad', 0)\n",
        "\n",
        "    N, C, H, W = x.shape\n",
        "    F, _, HH, WW = w.shape\n",
        "    _, _, H_out, W_out = dout.shape\n",
        "\n",
        "    # Pad the input\n",
        "    x_padded = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant')\n",
        "\n",
        "    # Initialize gradients\n",
        "    dx_padded = np.zeros_like(x_padded)\n",
        "    dw = np.zeros_like(w)\n",
        "    db = np.zeros_like(b)\n",
        "\n",
        "    # Calculate gradient with respect to bias\n",
        "    for f in range(F):\n",
        "        db[f] = np.sum(dout[:, f, :, :])\n",
        "\n",
        "    # Calculate gradient with respect to w and x\n",
        "    for n in range(N):\n",
        "        for f in range(F):\n",
        "            for h_out in range(H_out):\n",
        "                for w_out in range(W_out):\n",
        "                    h_start = h_out * stride\n",
        "                    w_start = w_out * stride\n",
        "                    dx_padded[n, :, h_start:h_start+HH, w_start:w_start+WW] += w[f] * dout[n, f, h_out, w_out]\n",
        "                    dw[f] += x_padded[n, :, h_start:h_start+HH, w_start:w_start+WW] * dout[n, f, h_out, w_out]\n",
        "\n",
        "    # Remove padding from dx\n",
        "    dx = dx_padded[:, :, pad:pad+H, pad:pad+W]\n",
        "\n",
        "    return dx, dw, db\n",
        "\n",
        "def max_pool_forward_naive(x, pool_param):\n",
        "    \"\"\"\n",
        "    A naive implementation of the forward pass for a max-pooling layer.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C, H, W)\n",
        "    - pool_param: dictionary with the following keys:\n",
        "      - 'pool_height': The height of each pooling region\n",
        "      - 'pool_width': The width of each pooling region\n",
        "      - 'stride': The distance between adjacent pooling regions\n",
        "\n",
        "    No padding is necessary here. Output size is given by\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output data, of shape (N, C, H', W') where H' and W' are given by\n",
        "      H' = 1 + (H - pool_height) / stride\n",
        "      W' = 1 + (W - pool_width) / stride\n",
        "    - cache: (x, pool_param)\n",
        "    \"\"\"\n",
        "    pool_height = pool_param.get('pool_height', 2)\n",
        "    pool_width = pool_param.get('pool_width', 2)\n",
        "    stride = pool_param.get('stride', 2)\n",
        "\n",
        "    N, C, H, W = x.shape\n",
        "\n",
        "    H_out = 1 + (H - pool_height) // stride\n",
        "    W_out = 1 + (W - pool_width) // stride\n",
        "\n",
        "    out = np.zeros((N, C, H_out, W_out))\n",
        "\n",
        "    for n in range(N):\n",
        "        for c in range(C):\n",
        "            for h_out in range(H_out):\n",
        "                for w_out in range(W_out):\n",
        "                    h_start = h_out * stride\n",
        "                    w_start = w_out * stride\n",
        "                    x_slice = x[n, c, h_start:h_start+pool_height, w_start:w_start+pool_width]\n",
        "                    out[n, c, h_out, w_out] = np.max(x_slice)\n",
        "\n",
        "    cache = (x, pool_param)\n",
        "    return out, cache\n",
        "\n",
        "def max_pool_backward_naive(dout, cache):\n",
        "    \"\"\"\n",
        "    A naive implementation of the backward pass for a max-pooling layer.\n",
        "\n",
        "    Inputs:\n",
        "    - dout: Upstream derivatives\n",
        "    - cache: A tuple of (x, pool_param) as in the forward pass.\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    x, pool_param = cache\n",
        "\n",
        "    pool_height = pool_param.get('pool_height', 2)\n",
        "    pool_width = pool_param.get('pool_width', 2)\n",
        "    stride = pool_param.get('stride', 2)\n",
        "\n",
        "    N, C, H, W = x.shape\n",
        "    _, _, H_out, W_out = dout.shape\n",
        "\n",
        "    dx = np.zeros_like(x)\n",
        "\n",
        "    for n in range(N):\n",
        "        for c in range(C):\n",
        "            for h_out in range(H_out):\n",
        "                for w_out in range(W_out):\n",
        "                    h_start = h_out * stride\n",
        "                    w_start = w_out * stride\n",
        "\n",
        "                    x_slice = x[n, c, h_start:h_start+pool_height, w_start:w_start+pool_width]\n",
        "                    max_idx = np.unravel_index(np.argmax(x_slice), x_slice.shape)\n",
        "\n",
        "                    dx[n, c, h_start+max_idx[0], w_start+max_idx[1]] += dout[n, c, h_out, w_out]\n",
        "\n",
        "    return dx\n",
        "\n",
        "def relu_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - x: Inputs, of any shape\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output, of the same shape as x\n",
        "    - cache: x\n",
        "    \"\"\"\n",
        "    out = np.maximum(0, x)\n",
        "    cache = x\n",
        "    return out, cache\n",
        "\n",
        "def relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: Input x, of same shape as dout\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    x = cache\n",
        "    dx = dout * (x > 0)\n",
        "    return dx\n",
        "\n",
        "def affine_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for an affine (fully-connected) layer.\n",
        "\n",
        "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
        "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
        "    then transform it to an output vector of dimension M.\n",
        "\n",
        "    Inputs:\n",
        "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
        "    - w: A numpy array of weights, of shape (D, M)\n",
        "    - b: A numpy array of biases, of shape (M,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, M)\n",
        "    - cache: (x, w, b)\n",
        "    \"\"\"\n",
        "    N = x.shape[0]\n",
        "    D = np.prod(x.shape[1:])\n",
        "    x_reshaped = x.reshape(N, D)\n",
        "    out = x_reshaped.dot(w) + b\n",
        "    cache = (x, w, b)\n",
        "    return out, cache\n",
        "\n",
        "def affine_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for an affine layer.\n",
        "\n",
        "    Inputs:\n",
        "    - dout: Upstream derivative, of shape (N, M)\n",
        "    - cache: Tuple of:\n",
        "      - x: Input data, of shape (N, d_1, ... d_k)\n",
        "      - w: Weights, of shape (D, M)\n",
        "      - b: Biases, of shape (M,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
        "    - dw: Gradient with respect to w, of shape (D, M)\n",
        "    - db: Gradient with respect to b, of shape (M,)\n",
        "    \"\"\"\n",
        "    x, w, b = cache\n",
        "    N = x.shape[0]\n",
        "    D = np.prod(x.shape[1:])\n",
        "    x_reshaped = x.reshape(N, D)\n",
        "\n",
        "    dx_reshaped = dout.dot(w.T)\n",
        "    dx = dx_reshaped.reshape(x.shape)\n",
        "    dw = x_reshaped.T.dot(dout)\n",
        "    db = np.sum(dout, axis=0)\n",
        "\n",
        "    return dx, dw, db"
      ],
      "metadata": {
        "id": "Nvn5F_4e6gUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropout implementation for regularization\n",
        "\n",
        "def dropout_forward(x, dropout_param):\n",
        "    \"\"\"\n",
        "    Performs the forward pass for dropout.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data, of any shape\n",
        "    - dropout_param: A dictionary with the following keys:\n",
        "      - p: Dropout parameter. We keep each neuron output with probability p.\n",
        "      - mode: 'test' or 'train'. If the mode is train, then perform dropout;\n",
        "        if the mode is test, then just return the input.\n",
        "      - seed: Seed for the random number generator. Passing seed makes this\n",
        "        function deterministic, which is needed for gradient checking but not\n",
        "        in real networks.\n",
        "\n",
        "    Outputs:\n",
        "    - out: Array of the same shape as x.\n",
        "    - cache: tuple (dropout_param, mask). In training mode, mask is the dropout\n",
        "      mask that was used to multiply the input; in test mode, mask is None.\n",
        "    \"\"\"\n",
        "    p, mode = dropout_param['p'], dropout_param['mode']\n",
        "    if 'seed' in dropout_param:\n",
        "        np.random.seed(dropout_param['seed'])\n",
        "\n",
        "    mask = None\n",
        "    out = None\n",
        "\n",
        "    if mode == 'train':\n",
        "        mask = (np.random.rand(*x.shape) < p) / p  # Scale the mask to preserve expected output\n",
        "        out = x * mask\n",
        "    elif mode == 'test':\n",
        "        out = x\n",
        "\n",
        "    cache = (dropout_param, mask)\n",
        "    return out, cache\n",
        "\n",
        "def dropout_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Perform the backward pass for dropout.\n",
        "\n",
        "    Inputs:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: (dropout_param, mask) from dropout_forward.\n",
        "    \"\"\"\n",
        "    dropout_param, mask = cache\n",
        "    mode = dropout_param['mode']\n",
        "\n",
        "    dx = None\n",
        "    if mode == 'train':\n",
        "        dx = dout * mask\n",
        "    elif mode == 'test':\n",
        "        dx = dout\n",
        "    return dx"
      ],
      "metadata": {
        "id": "896EZ7TR6uPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch normalization implementation\n",
        "\n",
        "def batchnorm_forward(x, gamma, beta, bn_param):\n",
        "    \"\"\"\n",
        "    Forward pass for batch normalization.\n",
        "\n",
        "    Input:\n",
        "    - x: Data of shape (N, D)\n",
        "    - gamma: Scale parameter of shape (D,)\n",
        "    - beta: Shift parameter of shape (D,)\n",
        "    - bn_param: Dictionary with the following keys:\n",
        "      - mode: 'train' or 'test'\n",
        "      - eps: Constant for numerical stability\n",
        "      - momentum: Constant for moving average\n",
        "      - running_mean: Array of shape (D,) for running mean\n",
        "      - running_var: Array of shape (D,) for running variance\n",
        "\n",
        "    Returns:\n",
        "    - out: Normalized output of shape (N, D)\n",
        "    - cache: Tuple for backward pass\n",
        "    \"\"\"\n",
        "    mode = bn_param['mode']\n",
        "    eps = bn_param.get('eps', 1e-5)\n",
        "    momentum = bn_param.get('momentum', 0.9)\n",
        "\n",
        "    N, D = x.shape\n",
        "    running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype))\n",
        "    running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype))\n",
        "\n",
        "    out, cache = None, None\n",
        "    if mode == 'train':\n",
        "        # Step 1: Calculate mean\n",
        "        mu = np.mean(x, axis=0)\n",
        "\n",
        "        # Step 2: Calculate variance\n",
        "        var = np.var(x, axis=0)\n",
        "\n",
        "        # Step 3: Normalize\n",
        "        x_norm = (x - mu) / np.sqrt(var + eps)\n",
        "\n",
        "        # Step 4: Scale and shift\n",
        "        out = gamma * x_norm + beta\n",
        "\n",
        "        # Update running mean and variance\n",
        "        running_mean = momentum * running_mean + (1 - momentum) * mu\n",
        "        running_var = momentum * running_var + (1 - momentum) * var\n",
        "\n",
        "        # Cache for backward pass\n",
        "        cache = (x, x_norm, mu, var, gamma, beta, eps)\n",
        "\n",
        "    elif mode == 'test':\n",
        "        # Use running mean and variance for normalization\n",
        "        x_norm = (x - running_mean) / np.sqrt(running_var + eps)\n",
        "        out = gamma * x_norm + beta\n",
        "\n",
        "    # Store updated running mean and var\n",
        "    bn_param['running_mean'] = running_mean\n",
        "    bn_param['running_var'] = running_var\n",
        "\n",
        "    return out, cache\n",
        "\n",
        "def batchnorm_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Backward pass for batch normalization.\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of shape (N, D)\n",
        "    - cache: Variable of intermediates from batchnorm_forward.\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to inputs x, of shape (N, D)\n",
        "    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n",
        "    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n",
        "    \"\"\"\n",
        "    x, x_norm, mu, var, gamma, beta, eps = cache\n",
        "    N, D = x.shape\n",
        "\n",
        "    # Gradient with respect to beta and gamma\n",
        "    dbeta = np.sum(dout, axis=0)\n",
        "    dgamma = np.sum(dout * x_norm, axis=0)\n",
        "\n",
        "    # Gradient with respect to x_norm\n",
        "    dx_norm = dout * gamma\n",
        "\n",
        "    # Gradient with respect to x\n",
        "    dvar = np.sum(dx_norm * (x - mu) * -0.5 * (var + eps)**(-1.5), axis=0)\n",
        "    dmu = np.sum(-dx_norm / np.sqrt(var + eps), axis=0) + dvar * np.mean(-2 * (x - mu), axis=0)\n",
        "\n",
        "    dx = dx_norm / np.sqrt(var + eps) + dvar * 2 * (x - mu) / N + dmu / N\n",
        "\n",
        "    return dx, dgamma, dbeta\n",
        "\n",
        "def spatial_batchnorm_forward(x, gamma, beta, bn_param):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for spatial batch normalization.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data of shape (N, C, H, W)\n",
        "    - gamma: Scale parameter, of shape (C,)\n",
        "    - beta: Shift parameter, of shape (C,)\n",
        "    - bn_param: Dictionary with the following keys:\n",
        "      - mode: 'train' or 'test'; required\n",
        "      - eps: Constant for numeric stability\n",
        "      - momentum: Constant for running mean / variance. momentum=0 means that\n",
        "        old information is discarded completely at every time step, while\n",
        "        momentum=1 means that new information is never incorporated. The\n",
        "        default of momentum=0.9 should work well in most situations.\n",
        "      - running_mean: Array of shape (C,) giving running mean of features\n",
        "      - running_var Array of shape (C,) giving running variance of features\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output data, of shape (N, C, H, W)\n",
        "    - cache: Values needed for the backward pass\n",
        "    \"\"\"\n",
        "    N, C, H, W = x.shape\n",
        "\n",
        "    # Reshape x to N*H*W x C to apply batch normalization\n",
        "    x_reshaped = x.transpose(0, 2, 3, 1).reshape(-1, C)\n",
        "\n",
        "    # Apply batch normalization\n",
        "    out_reshaped, cache = batchnorm_forward(x_reshaped, gamma, beta, bn_param)\n",
        "\n",
        "    # Reshape back to N x C x H x W\n",
        "    out = out_reshaped.reshape(N, H, W, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "    return out, cache\n",
        "\n",
        "def spatial_batchnorm_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for spatial batch normalization.\n",
        "\n",
        "    Inputs:\n",
        "    - dout: Upstream derivatives, of shape (N, C, H, W)\n",
        "    - cache: Values from the forward pass\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to inputs, of shape (N, C, H, W)\n",
        "    - dgamma: Gradient with respect to scale parameter, of shape (C,)\n",
        "    - dbeta: Gradient with respect to shift parameter, of shape (C,)\n",
        "    \"\"\"\n",
        "    N, C, H, W = dout.shape\n",
        "\n",
        "    # Reshape dout for batch normalization backward\n",
        "    dout_reshaped = dout.transpose(0, 2, 3, 1).reshape(-1, C)\n",
        "\n",
        "    # Apply batch normalization backward\n",
        "    dx_reshaped, dgamma, dbeta = batchnorm_backward(dout_reshaped, cache)\n",
        "\n",
        "    # Reshape dx back to N x C x H x W\n",
        "    dx = dx_reshaped.reshape(N, H, W, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "    return dx, dgamma, dbeta"
      ],
      "metadata": {
        "id": "34hIH5076xPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation functions\n",
        "\n",
        "def random_flips(X):\n",
        "    \"\"\"\n",
        "    Randomly flip images horizontally.\n",
        "\n",
        "    Input:\n",
        "    - X: Batch of images, of shape (N, C, H, W)\n",
        "\n",
        "    Returns:\n",
        "    - Augmented batch of images of the same shape\n",
        "    \"\"\"\n",
        "    out = np.zeros_like(X)\n",
        "    for i in range(X.shape[0]):\n",
        "        if np.random.rand() < 0.5:\n",
        "            out[i] = X[i, :, :, ::-1]  # Flip horizontally\n",
        "        else:\n",
        "            out[i] = X[i]\n",
        "    return out\n",
        "\n",
        "def random_crops(X, crop_shape, padding=4):\n",
        "    \"\"\"\n",
        "    Randomly crop images with padding.\n",
        "\n",
        "    Input:\n",
        "    - X: Batch of images, of shape (N, C, H, W)\n",
        "    - crop_shape: Shape of the crop (height, width)\n",
        "    - padding: Amount of padding to add before cropping\n",
        "\n",
        "    Returns:\n",
        "    - Augmented batch of images\n",
        "    \"\"\"\n",
        "    N, C, H, W = X.shape\n",
        "    H_crop, W_crop = crop_shape\n",
        "\n",
        "    # Add padding\n",
        "    padded = np.pad(X, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='constant')\n",
        "\n",
        "    out = np.zeros((N, C, H_crop, W_crop))\n",
        "\n",
        "    for i in range(N):\n",
        "        # Random crop position\n",
        "        h_start = np.random.randint(0, padded.shape[2] - H_crop + 1)\n",
        "        w_start = np.random.randint(0, padded.shape[3] - W_crop + 1)\n",
        "\n",
        "        # Extract crop\n",
        "        out[i] = padded[i, :, h_start:h_start+H_crop, w_start:w_start+W_crop]\n",
        "\n",
        "    return out\n",
        "\n",
        "def data_augmentation(X, y):\n",
        "    \"\"\"\n",
        "    Apply multiple data augmentation techniques.\n",
        "\n",
        "    Inputs:\n",
        "    - X: Batch of images, of shape (N, C, H, W)\n",
        "    - y: Labels, of shape (N,)\n",
        "\n",
        "    Returns:\n",
        "    - Augmented data and labels\n",
        "    \"\"\"\n",
        "    X_aug = random_flips(X)\n",
        "    X_aug = random_crops(X_aug, (X.shape[2], X.shape[3]), padding=4)\n",
        "\n",
        "    return X_aug, y"
      ],
      "metadata": {
        "id": "eS8-GSwa62Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN model implementation\n",
        "\n",
        "class ConvNet:\n",
        "    \"\"\"\n",
        "    A Convolutional Network architecture for image classification.\n",
        "    The architecture is based on:\n",
        "    [conv - spatial batch norm - relu - max pool] x 2 - [affine - batch norm - relu - dropout] - [affine - softmax]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=(3, 32, 32), num_filters=[32, 64], filter_size=3,\n",
        "                 hidden_dim=100, num_classes=10, weight_scale=1e-3, reg=0.0,\n",
        "                 dropout=0.5, use_batchnorm=True, dtype=np.float32, seed=None):\n",
        "        \"\"\"\n",
        "        Initialize a new ConvNet.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: Tuple (C, H, W) giving size of input data\n",
        "        - num_filters: List of number of filters for each convolutional layer\n",
        "        - filter_size: Size of filters to use\n",
        "        - hidden_dim: Number of units in the hidden affine layer\n",
        "        - num_classes: Number of classes for classification\n",
        "        - weight_scale: Scalar giving standard deviation for random weight initialization\n",
        "        - reg: Scalar giving L2 regularization strength\n",
        "        - dropout: Scalar between 0 and 1 giving dropout strength\n",
        "        - use_batchnorm: Whether to use batch normalization\n",
        "        - dtype: numpy datatype to use for computation\n",
        "        - seed: If not None, then seed for random number generator\n",
        "        \"\"\"\n",
        "        self.use_batchnorm = use_batchnorm\n",
        "        self.use_dropout = dropout > 0\n",
        "        self.reg = reg\n",
        "        self.num_layers = 2 + len(num_filters)\n",
        "        self.dtype = dtype\n",
        "        self.params = {}\n",
        "        self.bn_params = {}\n",
        "\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        C, H, W = input_dim\n",
        "\n",
        "        # Initialize first convolutional layer\n",
        "        self.params['W1'] = np.random.normal(0, weight_scale, (num_filters[0], C, filter_size, filter_size))\n",
        "        self.params['b1'] = np.zeros(num_filters[0])\n",
        "\n",
        "        # Initialize batch norm parameters if needed\n",
        "        if use_batchnorm:\n",
        "            self.params['gamma1'] = np.ones(num_filters[0])\n",
        "            self.params['beta1'] = np.zeros(num_filters[0])\n",
        "            self.bn_params['bn1'] = {'mode': 'train', 'eps': 1e-5, 'momentum': 0.9}\n",
        "\n",
        "        # Calculate input dimensions for the next layer\n",
        "        H_pool = 1 + (H - 2) // 2  # After pooling\n",
        "        W_pool = 1 + (W - 2) // 2  # After pooling\n",
        "\n",
        "        # Initialize second convolutional layer\n",
        "        self.params['W2'] = np.random.normal(0, weight_scale, (num_filters[1], num_filters[0], filter_size, filter_size))\n",
        "        self.params['b2'] = np.zeros(num_filters[1])\n",
        "\n",
        "        # Initialize batch norm parameters for second conv layer if needed\n",
        "        if use_batchnorm:\n",
        "            self.params['gamma2'] = np.ones(num_filters[1])\n",
        "            self.params['beta2'] = np.zeros(num_filters[1])\n",
        "            self.bn_params['bn2'] = {'mode': 'train', 'eps': 1e-5, 'momentum': 0.9}\n",
        "\n",
        "        # Calculate input dimensions for the fully connected layer\n",
        "        H_pool2 = 1 + (H_pool - 2) // 2  # After second pooling\n",
        "        W_pool2 = 1 + (W_pool - 2) // 2  # After second pooling\n",
        "        fc_input_dim = num_filters[1] * H_pool2 * W_pool2\n",
        "\n",
        "        # Initialize first fully connected layer\n",
        "        self.params['W3'] = np.random.normal(0, weight_scale, (fc_input_dim, hidden_dim))\n",
        "        self.params['b3'] = np.zeros(hidden_dim)\n",
        "\n",
        "        # Initialize batch norm parameters for first FC layer if needed\n",
        "        if use_batchnorm:\n",
        "            self.params['gamma3'] = np.ones(hidden_dim)\n",
        "            self.params['beta3'] = np.zeros(hidden_dim)\n",
        "            self.bn_params['bn3'] = {'mode': 'train', 'eps': 1e-5, 'momentum': 0.9}\n",
        "\n",
        "        # Initialize output layer\n",
        "        self.params['W4'] = np.random.normal(0, weight_scale, (hidden_dim, num_classes))\n",
        "        self.params['b4'] = np.zeros(num_classes)\n",
        "\n",
        "        # Initialize dropout params\n",
        "        self.dropout_params = {'mode': 'train', 'p': 1 - dropout}\n",
        "        if seed is not None:\n",
        "            self.dropout_params['seed'] = seed\n",
        "\n",
        "        # Cast all parameters to the correct datatype\n",
        "        for k, v in self.params.items():\n",
        "            self.params[k] = v.astype(dtype)\n",
        "\n",
        "    def loss(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Evaluate loss and gradient for the ConvNet.\n",
        "\n",
        "        Input / Output: Same API as TwoLayerNet.\n",
        "        \"\"\"\n",
        "        X = X.astype(self.dtype)\n",
        "        mode = 'test' if y is None else 'train'\n",
        "\n",
        "        # Update BN params and dropout params based on mode\n",
        "        if self.use_batchnorm:\n",
        "            for key, bn_param in self.bn_params.items():\n",
        "                bn_param['mode'] = mode\n",
        "        if self.use_dropout:\n",
        "            self.dropout_params['mode'] = mode\n",
        "\n",
        "        scores = None\n",
        "\n",
        "        # Forward pass through the network\n",
        "        conv1_out, conv1_cache = conv_forward_naive(X, self.params['W1'], self.params['b1'], {'stride': 1, 'pad': 1})\n",
        "\n",
        "        # Batch normalization for first conv layer if used\n",
        "        if self.use_batchnorm:\n",
        "            conv1_out, bn1_cache = spatial_batchnorm_forward(conv1_out, self.params['gamma1'],\n",
        "                                                             self.params['beta1'], self.bn_params['bn1'])\n",
        "\n",
        "        # ReLU activation for first conv layer\n",
        "        relu1_out, relu1_cache = relu_forward(conv1_out)\n",
        "\n",
        "        # Max pooling for first conv layer\n",
        "        pool1_out, pool1_cache = max_pool_forward_naive(relu1_out, {'pool_height': 2, 'pool_width': 2, 'stride': 2})\n",
        "\n",
        "        # Second convolutional layer\n",
        "        conv2_out, conv2_cache = conv_forward_naive(pool1_out, self.params['W2'], self.params['b2'], {'stride': 1, 'pad': 1})\n",
        "\n",
        "        # Batch normalization for second conv layer if used\n",
        "        if self.use_batchnorm:\n",
        "            conv2_out, bn2_cache = spatial_batchnorm_forward(conv2_out, self.params['gamma2'],\n",
        "                                                             self.params['beta2'], self.bn_params['bn2'])\n",
        "\n",
        "        # ReLU activation for second conv layer\n",
        "        relu2_out, relu2_cache = relu_forward(conv2_out)\n",
        "\n",
        "        # Max pooling for second conv layer\n",
        "        pool2_out, pool2_cache = max_pool_forward_naive(relu2_out, {'pool_height': 2, 'pool_width': 2, 'stride': 2})\n",
        "\n",
        "        # First fully connected layer\n",
        "        fc1_out, fc1_cache = affine_forward(pool2_out, self.params['W3'], self.params['b3'])\n",
        "\n",
        "        # Batch normalization for first fc layer if used\n",
        "        if self.use_batchnorm:\n",
        "            fc1_out, bn3_cache = batchnorm_forward(fc1_out, self.params['gamma3'],\n",
        "                                                  self.params['beta3'], self.bn_params['bn3'])\n",
        "\n",
        "        # ReLU activation for first fc layer\n",
        "        relu3_out, relu3_cache = relu_forward(fc1_out)\n",
        "\n",
        "        # Dropout after first fc layer if used\n",
        "        if self.use_dropout:\n",
        "            relu3_out, dropout_cache = dropout_forward(relu3_out, self.dropout_params)\n",
        "\n",
        "        # Output layer\n",
        "        scores, fc2_cache = affine_forward(relu3_out, self.params['W4'], self.params['b4'])\n",
        "\n",
        "        # If test mode return scores\n",
        "        if mode == 'test':\n",
        "            return scores\n",
        "\n",
        "        # Calculate loss and gradients\n",
        "        loss, dscores = softmax_loss(scores, y)\n",
        "\n",
        "        # Add regularization\n",
        "        loss += 0.5 * self.reg * (np.sum(self.params['W1'] ** 2) + np.sum(self.params['W2'] ** 2) +\n",
        "                                 np.sum(self.params['W3'] ** 2) + np.sum(self.params['W4'] ** 2))\n",
        "\n",
        "        # Backward pass\n",
        "        grads = {}\n",
        "\n",
        "        # Backprop through output layer\n",
        "        dx4, grads['W4'], grads['b4'] = affine_backward(dscores, fc2_cache)\n",
        "\n",
        "        # Add regularization gradient for W4\n",
        "        grads['W4'] += self.reg * self.params['W4']\n",
        "\n",
        "        # Backprop through dropout if used\n",
        "        if self.use_dropout:\n",
        "            dx4 = dropout_backward(dx4, dropout_cache)\n",
        "\n",
        "        # Backprop through ReLU3\n",
        "        dx3 = relu_backward(dx4, relu3_cache)\n",
        "\n",
        "        # Backprop through batch norm if used\n",
        "        if self.use_batchnorm:\n",
        "            dx3, grads['gamma3'], grads['beta3'] = batchnorm_backward(dx3, bn3_cache)\n",
        "\n",
        "        # Backprop through FC1\n",
        "        dx2, grads['W3'], grads['b3'] = affine_backward(dx3, fc1_cache)\n",
        "\n",
        "        # Add regularization gradient for W3\n",
        "        grads['W3'] += self.reg * self.params['W3']\n",
        "\n",
        "        # Backprop through second pooling layer\n",
        "        dx2 = max_pool_backward_naive(dx2, pool2_cache)\n",
        "\n",
        "        # Backprop through second ReLU\n",
        "        dx2 = relu_backward(dx2, relu2_cache)\n",
        "\n",
        "        # Backprop through second batch norm if used\n",
        "        if self.use_batchnorm:\n",
        "            dx2, grads['gamma2'], grads['beta2'] = spatial_batchnorm_backward(dx2, bn2_cache)\n",
        "\n",
        "        # Backprop through second conv layer\n",
        "        dx1, grads['W2'], grads['b2'] = conv_backward_naive(dx2, conv2_cache)\n",
        "\n",
        "        # Add regularization gradient for W2\n",
        "        grads['W2'] += self.reg * self.params['W2']\n",
        "\n",
        "        # Backprop through first pooling layer\n",
        "        dx1 = max_pool_backward_naive(dx1, pool1_cache)\n",
        "\n",
        "        # Backprop through first ReLU\n",
        "        dx1 = relu_backward(dx1, relu1_cache)\n",
        "\n",
        "        # Backprop through first batch norm if used\n",
        "        if self.use_batchnorm:\n",
        "            dx1, grads['gamma1'], grads['beta1'] = spatial_batchnorm_backward(dx1, bn1_cache)\n",
        "\n",
        "        # Backprop through first conv layer\n",
        "        dx, grads['W1'], grads['b1'] = conv_backward_naive(dx1, conv1_cache)\n",
        "\n",
        "        # Add regularization gradient for W1\n",
        "        grads['W1'] += self.reg * self.params['W1']\n",
        "\n",
        "        return loss, grads\n",
        "\n",
        "def softmax_loss(x, y):\n",
        "    \"\"\"\n",
        "    Computes the loss and gradient for softmax classification.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
        "      class for the ith input.\n",
        "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
        "      0 <= y[i] < C\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss: Scalar giving the loss\n",
        "    - dx: Gradient of the loss with respect to x\n",
        "    \"\"\"\n",
        "    shifted_logits = x - np.max(x, axis=1, keepdims=True)\n",
        "    Z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True)\n",
        "    log_probs = shifted_logits - np.log(Z)\n",
        "    probs = np.exp(log_probs)\n",
        "    N = x.shape[0]\n",
        "    loss = -np.sum(log_probs[np.arange(N), y]) / N\n",
        "    dx = probs.copy()\n",
        "    dx[np.arange(N), y] -= 1\n",
        "    dx /= N\n",
        "    return loss, dx"
      ],
      "metadata": {
        "id": "l-w5D_Lq65eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solver class for training the model\n",
        "\n",
        "class Solver:\n",
        "    \"\"\"\n",
        "    A Solver encapsulates all the logic necessary for training classification\n",
        "    models. The Solver performs stochastic gradient descent using different\n",
        "    update rules.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, data, **kwargs):\n",
        "        \"\"\"\n",
        "        Construct a new Solver instance.\n",
        "\n",
        "        Required:\n",
        "        - model: A model object conforming to the API described above\n",
        "        - data: A dictionary of training and validation data\n",
        "\n",
        "        Optional kwargs:\n",
        "        - update_rule: A function that updates model parameters\n",
        "        - optim_config: A dictionary for the update rule\n",
        "        - lr_decay: A scalar for learning rate decay\n",
        "        - batch_size: Size of minibatches used to compute loss and gradient\n",
        "        - num_epochs: The number of epochs to run for\n",
        "        - print_every: Integer, print loss every X iterations\n",
        "        - verbose: Boolean, print progress during optimization\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.X_train = data['X_train']\n",
        "        self.y_train = data['y_train']\n",
        "        self.X_val = data['X_val']\n",
        "        self.y_val = data['y_val']\n",
        "\n",
        "        # Unpack keyword arguments\n",
        "        self.update_rule = kwargs.pop('update_rule', self._sgd)\n",
        "        self.optim_config = kwargs.pop('optim_config', {})\n",
        "        self.lr_decay = kwargs.pop('lr_decay', 1.0)\n",
        "        self.batch_size = kwargs.pop('batch_size', 100)\n",
        "        self.num_epochs = kwargs.pop('num_epochs', 10)\n",
        "        self.print_every = kwargs.pop('print_every', 10)\n",
        "        self.verbose = kwargs.pop('verbose', True)\n",
        "\n",
        "        # Set default values for optimizer configuration\n",
        "        if 'learning_rate' not in self.optim_config:\n",
        "            self.optim_config['learning_rate'] = 1e-3\n",
        "        self.optim_config_history = {}\n",
        "\n",
        "        # Keep track of history\n",
        "        self.loss_history = []\n",
        "        self.train_acc_history = []\n",
        "        self.val_acc_history = []\n",
        "\n",
        "        # Make a deep copy of the optim_config for each parameter\n",
        "        self.optim_configs = {}\n",
        "        for p in self.model.params:\n",
        "            d = {k: v for k, v in self.optim_config.items()}\n",
        "            self.optim_configs[p] = d\n",
        "            # Keep a history of the config parameters\n",
        "            self.optim_config_history[p] = []\n",
        "\n",
        "    def _sgd(self, w, dw, config=None):\n",
        "        \"\"\"\n",
        "        Performs vanilla stochastic gradient descent.\n",
        "\n",
        "        config dict format:\n",
        "        - learning_rate: Scalar learning rate\n",
        "        \"\"\"\n",
        "        if config is None: config = {}\n",
        "        config.setdefault('learning_rate', 1e-2)\n",
        "\n",
        "        w -= config['learning_rate'] * dw\n",
        "        return w, config\n",
        "\n",
        "    def _sgd_momentum(self, w, dw, config=None):\n",
        "        \"\"\"\n",
        "        Performs SGD with momentum.\n",
        "\n",
        "        config dict format:\n",
        "        - learning_rate: Scalar learning rate\n",
        "        - momentum: Scalar between 0 and 1 giving the momentum value\n",
        "        - velocity: A numpy array of the same shape as w and dw used to store\n",
        "          the velocity\n",
        "        \"\"\"\n",
        "        if config is None: config = {}\n",
        "        config.setdefault('learning_rate', 1e-2)\n",
        "        config.setdefault('momentum', 0.9)\n",
        "        v = config.get('velocity', np.zeros_like(w))\n",
        "\n",
        "        v = config['momentum'] * v - config['learning_rate'] * dw\n",
        "        next_w = w + v\n",
        "        config['velocity'] = v\n",
        "\n",
        "        return next_w, config\n",
        "\n",
        "    def _adam(self, w, dw, config=None):\n",
        "        \"\"\"\n",
        "        Uses the Adam update rule, which incorporates moving averages of both the\n",
        "        gradient and its square and a bias correction term.\n",
        "\n",
        "        config dict format:\n",
        "        - learning_rate: Scalar learning rate\n",
        "        - beta1: Decay rate for moving average of first moment of gradient\n",
        "        - beta2: Decay rate for moving average of second moment of gradient\n",
        "        - epsilon: Small scalar used for numerical stability\n",
        "        - m: Moving average of gradient\n",
        "        - v: Moving average of squared gradient\n",
        "        - t: Iteration number\n",
        "        \"\"\"\n",
        "        if config is None: config = {}\n",
        "        config.setdefault('learning_rate', 1e-3)\n",
        "        config.setdefault('beta1', 0.9)\n",
        "        config.setdefault('beta2', 0.999)\n",
        "        config.setdefault('epsilon', 1e-8)\n",
        "        config.setdefault('m', np.zeros_like(w))\n",
        "        config.setdefault('v', np.zeros_like(w))\n",
        "        config.setdefault('t', 0)\n",
        "\n",
        "        config['t'] += 1\n",
        "        config['m'] = config['beta1'] * config['m'] + (1 - config['beta1']) * dw\n",
        "        config['v'] = config['beta2'] * config['v'] + (1 - config['beta2']) * (dw * dw)\n",
        "\n",
        "        # Bias correction\n",
        "        mb = config['m'] / (1 - config['beta1'] ** config['t'])\n",
        "        vb = config['v'] / (1 - config['beta2'] ** config['t'])\n",
        "\n",
        "        next_w = w - config['learning_rate'] * mb / (np.sqrt(vb) + config['epsilon'])\n",
        "\n",
        "        return next_w, config\n",
        "\n",
        "    def _rmsprop(self, w, dw, config=None):\n",
        "        \"\"\"\n",
        "        Uses the RMSProp update rule, which uses a moving average of squared gradient values\n",
        "        to set adaptive learning rates.\n",
        "\n",
        "        config format:\n",
        "        - learning_rate: Scalar learning rate\n",
        "        - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared gradient cache\n",
        "        - epsilon: Small scalar used for numerical stability\n",
        "        - cache: Moving average of second moments of gradients\n",
        "        \"\"\"\n",
        "        if config is None: config = {}\n",
        "        config.setdefault('learning_rate', 1e-2)\n",
        "        config.setdefault('decay_rate', 0.99)\n",
        "        config.setdefault('epsilon', 1e-8)\n",
        "        config.setdefault('cache', np.zeros_like(w))\n",
        "\n",
        "        config['cache'] = config['decay_rate'] * config['cache'] + (1 - config['decay_rate']) * (dw ** 2)\n",
        "        next_w = w - config['learning_rate'] * dw / (np.sqrt(config['cache']) + config['epsilon'])\n",
        "\n",
        "        return next_w, config\n",
        "\n",
        "    def _step(self, X_batch, y_batch):\n",
        "        \"\"\"\n",
        "        Make a single gradient update with data augmentation.\n",
        "        \"\"\"\n",
        "        # Apply data augmentation\n",
        "        X_batch_aug, y_batch = data_augmentation(X_batch, y_batch)\n",
        "\n",
        "        # Compute loss and gradients\n",
        "        loss, grads = self.model.loss(X_batch_aug, y_batch)\n",
        "        self.loss_history.append(loss)\n",
        "\n",
        "        # Apply parameter updates\n",
        "        for p, w in self.model.params.items():\n",
        "            dw = grads[p]\n",
        "            config = self.optim_configs[p]\n",
        "            next_w, next_config = self.update_rule(w, dw, config)\n",
        "            self.model.params[p] = next_w\n",
        "            self.optim_configs[p] = next_config\n",
        "\n",
        "            # Record config history\n",
        "            self.optim_config_history[p].append({k: v for k, v in next_config.items() if k != 'velocity' and k != 'm' and k != 'v' and k != 'cache'})\n",
        "\n",
        "    def check_accuracy(self, X, y, num_samples=None, batch_size=100):\n",
        "        \"\"\"\n",
        "        Check accuracy of the model on the provided data.\n",
        "\n",
        "        - X: Array of data, of shape (N, d_1, ..., d_k)\n",
        "        - y: Array of labels, of shape (N,)\n",
        "        - num_samples: If not None, subsample the data and only test on num_samples points\n",
        "        - batch_size: Split X and y into batches of this size to avoid using too much memory\n",
        "\n",
        "        Returns:\n",
        "        - accuracy: Scalar giving the fraction of instances that were correctly classified\n",
        "        \"\"\"\n",
        "        N = X.shape[0]\n",
        "        if num_samples is not None and N > num_samples:\n",
        "            indices = np.random.choice(N, num_samples)\n",
        "            N = num_samples\n",
        "            X = X[indices]\n",
        "            y = y[indices]\n",
        "\n",
        "        # Compute predictions in batches\n",
        "        num_batches = N // batch_size\n",
        "        if N % batch_size != 0:\n",
        "            num_batches += 1\n",
        "\n",
        "        y_pred = []\n",
        "        for i in range(num_batches):\n",
        "            start = i * batch_size\n",
        "            end = min((i + 1) * batch_size, N)\n",
        "\n",
        "            batch_scores = self.model.loss(X[start:end])\n",
        "            batch_preds = np.argmax(batch_scores, axis=1)\n",
        "            y_pred.append(batch_preds)\n",
        "\n",
        "        y_pred = np.hstack(y_pred)\n",
        "        accuracy = np.mean(y_pred == y)\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Run optimization to train the model.\n",
        "        \"\"\"\n",
        "        num_train = self.X_train.shape[0]\n",
        "        iterations_per_epoch = max(num_train // self.batch_size, 1)\n",
        "        num_iterations = self.num_epochs * iterations_per_epoch\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            # Shuffle training data\n",
        "            shuffle_indices = np.random.permutation(num_train)\n",
        "            X_train_shuffled = self.X_train[shuffle_indices]\n",
        "            y_train_shuffled = self.y_train[shuffle_indices]\n",
        "\n",
        "            for i in range(iterations_per_epoch):\n",
        "                batch_start = i * self.batch_size\n",
        "                batch_end = min((i + 1) * self.batch_size, num_train)\n",
        "                X_batch = X_train_shuffled[batch_start:batch_end]\n",
        "                y_batch = y_train_shuffled[batch_start:batch_end]\n",
        "\n",
        "                self._step(X_batch, y_batch)\n",
        "\n",
        "                # Print training loss\n",
        "                if self.verbose and (i + 1) % self.print_every == 0:\n",
        "                    print('Epoch %d, iteration %d / %d: loss %f' % (\n",
        "                        epoch + 1, i + 1, iterations_per_epoch, self.loss_history[-1]))\n",
        "\n",
        "            # Decay learning rate\n",
        "            for k in self.optim_configs:\n",
        "                self.optim_configs[k]['learning_rate'] *= self.lr_decay\n",
        "\n",
        "            # Evaluate train and val accuracy\n",
        "            train_acc = self.check_accuracy(self.X_train, self.y_train, num_samples=1000)\n",
        "            val_acc = self.check_accuracy(self.X_val, self.y_val)\n",
        "            self.train_acc_history.append(train_acc)\n",
        "            self.val_acc_history.append(val_acc)\n",
        "\n",
        "            if self.verbose:\n",
        "                print('Epoch %d / %d: train acc: %f, val_acc: %f' % (\n",
        "                    epoch + 1, self.num_epochs, train_acc, val_acc))"
      ],
      "metadata": {
        "id": "eBQi6sWb7L8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluation\n",
        "\n",
        "# Load CIFAR-10 data\n",
        "data = get_CIFAR10_data()\n",
        "for k, v in data.items():\n",
        "    print('%s: ' % k, v.shape)\n",
        "\n",
        "# Create the model with the best hyperparameters\n",
        "model = ConvNet(input_dim=(3, 32, 32),\n",
        "                num_filters=[32, 64],\n",
        "                filter_size=3,\n",
        "                hidden_dim=500,\n",
        "                num_classes=10,\n",
        "                weight_scale=5e-2,\n",
        "                reg=0.001,\n",
        "                dropout=0.5,\n",
        "                use_batchnorm=True)\n",
        "\n",
        "# Create solver with Adam optimizer\n",
        "solver = Solver(model, data,\n",
        "                update_rule=Solver._adam,\n",
        "                optim_config={\n",
        "                    'learning_rate': 1e-3,\n",
        "                },\n",
        "                lr_decay=0.95,\n",
        "                batch_size=128,\n",
        "                num_epochs=10,\n",
        "                print_every=100,\n",
        "                verbose=True)\n",
        "\n",
        "# Train the model\n",
        "solver.train()\n",
        "\n",
        "# Evaluate final performance\n",
        "print('Final validation accuracy:', solver.val_acc_history[-1])\n",
        "print('Final test accuracy:', solver.check_accuracy(data['X_test'], data['y_test']))\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(solver.train_acc_history, '-o')\n",
        "plt.plot(solver.val_acc_history, '-o')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('Accuracy history')\n",
        "\n",
        "# Plot loss history\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(solver.loss_history)\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Loss history')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize predictions on test set\n",
        "def visualize_predictions(X, y, y_pred, classes, samples=8):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for i in range(samples):\n",
        "        plt.subplot(2, 4, i+1)\n",
        "        img = X[i].transpose(1, 2, 0)\n",
        "        img = img / 2 + 0.5  # Unnormalize\n",
        "        plt.imshow(img)\n",
        "        plt.title(f'True: {classes[y[i]]}\\nPred: {classes[y_pred[i]]}')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Get predictions on a few test images\n",
        "test_indices = np.random.choice(data['X_test'].shape[0], 8, replace=False)\n",
        "test_images = data['X_test'][test_indices]\n",
        "test_labels = data['y_test'][test_indices]\n",
        "test_scores = model.loss(test_images)\n",
        "test_preds = np.argmax(test_scores, axis=1)\n",
        "\n",
        "# Visualize predictions\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "visualize_predictions(test_images, test_labels, test_preds, classes)"
      ],
      "metadata": {
        "id": "hxDVi9rj7hK3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}